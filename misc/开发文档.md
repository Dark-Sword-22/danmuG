# 面向开发人员的文档

没有时间写详细文档，这里算是笔记与思路说明，大概分为几个部分，介绍本项目所含组件及其主要逻辑。

### 目前的组件成分

- `danmu.py` 这个组件本身可以挂机自动获取弹幕，直播结束后自动提交到github。
- `render.py` 这个组件与github actons连接，每次提交后会依据更新内容跑一些自然语言处理算法，分析高能时间。
- `.github` actions 文件夹，与pages服务配合，负责触发`render.py`并将结果展示到页面。
- `templates` 静态模板，用来修改展示页面的样式。
- `thunder` 投递同步文件夹，里面包含一套弹幕重投的服务端和客户端。
- `mkm` 副产物，可以挂机自动截留cc直播流。 

### danmu.py

基于aiohttp的自动弹幕机，可以实现无人值守获取与上传弹幕。它的主要api请参考`danmaku`文件夹。

它的主要实现有三（四）部分。（下文中将协程称为线程，为避免歧义，不再赘述）

- 主线程永远循环，没什么特别要做的，主要是捕捉用户启动与终止信号。
- 守护线程`observer`，这个线程的设计目的是希望以一种较低资源占用的方式扫描直播间是否在直播状态，默认每5秒扫描一次。起初我们设计时并未考虑这个线程，而是直接考虑有弹幕流就记录，弹幕流停止（直播停止后一段时间弹幕总是会停的）就认为直播结束并commit文件。但是经过测试发现cc的弹幕流与直播开启与否无关，在无直播时间同样可以黑屏板聊，这给原先的逻辑造成困难，所以有了调用另一个api的观察员线程。这个线程在程序执行中无法被摧毁。
- 守护线程`fisherman`，摸取弹幕的主体线程，它可以被设计为每次observer发现开播后新开一个线程实例负责本次直播的记录，也可以被设计为使用内存交互的互相触发的常开线程，本例中采用的是后者的设计。即存在开和关两种状态，开关则由`observer`负责调整。在开状态会进入弹幕记录状态。
- `Writer`，每当`fisherman`从休眠状态转为激活状态（即直播开启），会创建一个`Writer`实例，负责本次直播的弹幕记录。`Writer`本身有记录时间的转化功能，不具备过滤功能。即弹幕是否合法的检查在`fisherman.ddos_protect`发生，将合法字符串推入`Writer`，后者会负责将字符串转化为合法的时间戳+内容的格式。两者之间通过`MQueue`交互。

额外的说明：

关于弹幕过滤的逻辑。考虑到直播中经常有满屏问号和满屏草的情况，以及节奏风暴等等，再加之如果需要重投，B站爸爸的审核比较严格，因此直接压入`Writer`是不太合理的做法，所以有了事先过滤。弹幕过滤的逻辑分为两部分，第一部分是触发关键字模式匹配的肯定是直接过滤掉了，包括涉黄暴字符，以及cc的热词弹幕等等智力不太健全的东西。第二部分是拦截器，拦截器的目的是防洪，即不要在瞬时间向后端压入一大堆问号这种弹幕，没什么意义。拦截器需要实现的效果是一种动态滤波，即以频率角度来看，在低通滤波的基础上，滤波量与频率成正比。即瞬时输入越大削弱越大，这样既可以保留弹幕比较热点的区域，又能让它们不那么爆炸。详细逻辑请参代码内注释。

### render.py

基于gensim的NLP模块。不是NLP大佬，凑合写一写。

它的设计目的是希望以一种比较原始的方式尽量提取出弹幕关键字，因为目前我们使用的是基于github的CI服务器+pages静态页面分发的部署方式，虽然这个服务器是很强的，但是出于部署方式考量我觉得不合使用一些sota的，效果fansy但资源占用高的算法。

这里还是采用传统基于统计学的模式，分词还是采用jieba模块，之后是构建字典，doc2bow，然后使用lda模型训练。对热点区域进行相似度搜索，哪条弹幕的向量相似度最高则选取为代表弹幕。

经过观察它的输出结果似乎与输入顺序相关，训练模型存在不稳定性，我不知道怎么解决，如果有NLP大佬还请教我。

### templates

html模板采用基于cdn的单实例Vue，不愿花太多精力在画图上，具体的画图是通过echarts实现的，实现部分在上文的`render.py`当中，并不在模板当中。echarts是使用pyecharts由python直接传递渲染好的html，初期这么设计是出于快速开发的目的，后面随着需求逐渐增多才转移到Vue，不愿意再花精力使用echarts.js重构于是保留了下来。

Vue的样式采用的是同样的cdn嵌入的Buefy。

### thunder

弹幕重投发送组件，包含了基于异步的客户端和服务器套件，目前经过一系列测试没发现什么太大问题，应该可以比较稳定的运行了。目前我用手头资源架设了一个免费的协调服务器，但理论上这个模块设计的目的，当一个服务摸了以后，是任何人都可以组建自己的协调服务。

它采用一种简单的`Master+Slave`机制，考虑到弹幕重投并不需要那么多用户，这种简单设计总体是work的。我们设计的整体逻辑中，相当主要的一点是希望服务占用的资源尽可能的低，这样它可以被任何人以低成本架设，如果有人架设好服务器后，其他人想要参与的成本也会降低。

#### dmclient.py

这个组件是相当简单的一个组件，其中核心代码应该也就100行左右，剩下的看起来比较长是因为发现py自带的configparser不能识别百分号（这在阿B的sessiondata中存在）所以随手写了个parser，完全没有动用一个脑细胞，所以看起来比较冗长。

它实现的逻辑就是，开启运行后，按用户设定的模式（自动获取最新弹幕，或者获取指定视频的弹幕）,向协调服务器请求是否有任务，有任务就获取并投递，然后不断循环这个过程。在更新后加入了防洪睡眠，也就是说如果你让他长期在服务器上工作的话，虽然每条弹幕投递间隔很长，但累积下来并不少。比如为了防止每月投递10万条被封号之类的，加入了一个睡眠时间，每天只工作若干小时。当然了这种情况下为了保证投递效率当然是需要更多用户参与的。

一个标准投递流程是，client首先向协调服务器请求任务，如果存在任务的话服务器会返回应该投递的`bvid`，`cid`，时间，弹幕内容，以及针对此任务的token。client需要向服务器确认此任务已经被领取，因为单纯获取任务并不会改变其在后端的状态。之后client会投递该任务，并反复刷新弹幕列表，直到确认已经投递成功，然后回报给服务器提交任务已经完成。反复刷新的原因是为了防止阿B的某些用户群体拦截，即有些账号存在“虽然以我的视角看弹幕/评论完全发出去了，但其实全世界只有我能看到”的这种情况。不得不说也是大数据时代之恶了。

主要注意的是，原本主体完全采用异步网络设计，但是打包时发现aiohttp的post模块打包似乎有些问题，于是把出问题的部分又换成了同步的requests变成了一种奇怪的混用状态。不过考虑到本身程序的请求频次极低，这点倒是无伤大雅。

打包本身是为了向无编程环境/编程经验的用户分发程序，毕竟是需要众人拾柴火焰高的。我在打包过程中（使用pyinstaller做简单打包)发现lxml和hyper（这是requests用来解析http2.0的补丁）会出现识别问题，需要人工将site-package中的文件夹覆盖，我已经讲我使用的版本附加到`misc`文件夹中，如有其他人需要打包不再需要费力翻找。

#### dmserver.py

基于fastapi的单线程异步服务，部署采用`gunicorn + uvicorn + fastapi`的方式，在当前需求下，其极低的资源占用但同时能提供约1-10k数量级的请求处理能力可以完美的满足需求。后端基于sqlalchmy1.4（异步）的数据虚拟化，然后接到sqlite上。使用sqlite的好处是即使是想在一台500MB内存的机器搭建协调服务器也无需为mysql的内存占用而烦恼，目前看来各方面都还工作得不错。

为了管理方便，寻了个网页版sqlite可视化项目名叫phpliteadmin。另外为了开发方便，搞了个日志ws推送的接口，在debug阶段可以避免ssh连接，会轻松一些。

服务器的具体逻辑可以从`dmserver.py`开始看，其中DAL的部分在`dmdb.py`里完成，剩余还有一些小零件在`dmutils.py`里，总体是个非常小巧的项目。
